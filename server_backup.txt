
# ==========================================
# FILE: backend/server.py
# ==========================================

import os
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import json
import asyncio
from openai import OpenAI

# Import local MCP Manager
from mcp_manager import MCPManager

app = FastAPI(title="DellTech AI Workspace API", version="2.0.0")

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize MCP Manager
mcp_manager = MCPManager(config_path="mcp_config.json")

# Models
class Message(BaseModel):
    role: str
    content: str | None
    tool_call_id: str | None = None

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    stream: bool = False
    temperature: float = 0.7
    # Removed enable_tools flag, it is now enabled by default

class AddServerRequest(BaseModel):
    name: str
    command: str
    args: List[str] = []
    env: Dict[str, str] = {}

class ToggleServerRequest(BaseModel):
    enabled: bool

@app.on_event("startup")
async def startup_event():
    """Start MCP servers on backend startup with auto-discovery."""
    print("--- DellTech AI Backend Starting ---")
    
    # --- Auto-Configuration Logic ---
    config_path = "mcp_config.json"
    should_configure = False
    
    if not os.path.exists(config_path):
        should_configure = True
    else:
        try:
            with open(config_path, 'r') as f:
                data = json.load(f)
                if not data.get("servers"): should_configure = True
        except:
            should_configure = True

    if should_configure:
        print("[System] Configuring default MCP servers...")
        cwd = os.getcwd()
        # Look for the script in current or backend folder
        candidates = [
            os.path.join(cwd, "backend", "outlook_MCP_server.py"),
            os.path.join(cwd, "outlook_MCP_server.py")
        ]
        
        script_path = next((p for p in candidates if os.path.exists(p)), None)
        
        if script_path:
            # Normalize path for JSON
            script_path = script_path.replace("\\", "/")
            default_config = {
                "servers": {
                    "Outlook-Sync": {
                        "command": "python",
                        "args": [script_path],
                        "enabled": True,
                        "env": {}
                    }
                }
            }
            with open(config_path, "w") as f:
                json.dump(default_config, f, indent=2)
            print(f"[System] Auto-configured Outlook MCP at {script_path}")
        else:
            print("[System] Outlook script not found. Skipping auto-config.")
            
    await mcp_manager.start_all()

@app.on_event("shutdown")
async def shutdown_event():
    await mcp_manager.stop_all()

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.get("/mcp/servers")
async def list_servers():
    return mcp_manager.list_servers_status()

@app.post("/mcp/servers")
async def add_server(req: AddServerRequest):
    await mcp_manager.add_server(req.name, {"command": req.command, "args": req.args, "env": req.env, "enabled": True})
    return {"status": "added", "name": req.name}

@app.patch("/mcp/servers/{name}")
async def toggle_server(name: str, req: ToggleServerRequest):
    await mcp_manager.toggle_server(name, req.enabled)
    return {"status": "updated", "name": name, "enabled": req.enabled}

@app.delete("/mcp/servers/{name}")
async def remove_server(name: str):
    await mcp_manager.remove_server(name)
    return {"status": "removed", "name": name}

# --- Core Chat Logic ---

async def smart_stream_generator(request: ChatCompletionRequest, req: Request):
    """
    The Brain. Orchestrates the conversation between User, LLM, and MCP Tools.
    """
    api_key = req.headers.get("Authorization", "").replace("Bearer ", "")
    llm_base_url = req.headers.get("X-LLM-Base-URL", "https://api.openai.com/v1")
    
    # COMPATIBILITY FIX: Allow empty API keys for local LLMs (Ollama, LM Studio, etc)
    if not api_key: 
        api_key = "dummy-key-for-local-llm"

    try:
        client = OpenAI(api_key=api_key, base_url=llm_base_url)
        
        # 1. Fetch available tools dynamically from MCP Manager
        # Always fetch tools by default as per user request
        tools = await mcp_manager.get_all_tools_definitions()
        
        messages = [{"role": m.role, "content": m.content, "tool_call_id": m.tool_call_id} for m in request.messages]
        
        # System Prompt Injection if not present
        if messages and messages[0]['role'] != 'system':
            messages.insert(0, {"role": "system", "content": "You are a helpful Dell Tech assistant. You have access to tools. Use them whenever data is needed (email, calendar)."})

        # --- Round 1: Initial Query ---
        stream = client.chat.completions.create(
            model=request.model,
            messages=messages,
            tools=tools if tools else None,
            stream=True,
            temperature=request.temperature
        )
        
        tool_calls = []
        
        for chunk in stream:
            delta = chunk.choices[0].delta
            
            # Accumulate Tool Calls
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    if tc.index is not None:
                         while len(tool_calls) <= tc.index:
                             tool_calls.append({"id": "", "function": {"name": "", "arguments": ""}})
                         if tc.id: tool_calls[tc.index]["id"] = tc.id
                         if tc.function.name: tool_calls[tc.index]["function"]["name"] = tc.function.name
                         if tc.function.arguments: tool_calls[tc.index]["function"]["arguments"] += tc.function.arguments
            
            # Yield Content immediately to user
            if delta.content:
                yield f"data: {json.dumps(chunk.model_dump())}\n\n"

        # --- Round 2: Tool Execution (if needed) ---
        if tool_calls:
             # Append the Assistant's "Thought" (the tool call) to history
             messages.append({
                 "role": "assistant",
                 "content": None,
                 "tool_calls": tool_calls
             })
             
             for tc in tool_calls:
                 func_name = tc["function"]["name"]
                 call_id = tc["id"]
                 
                 # Notify Frontend: "I am starting a tool"
                 yield f"data: {json.dumps({'type': 'tool_start', 'tool': func_name})}\n\n"
                 
                 try:
                     args = json.loads(tc["function"]["arguments"])
                     print(f"[Orchestrator] Executing {func_name} with {args}")
                     
                     # CALL MCP
                     result_str = await mcp_manager.execute_tool(func_name, args)
                     
                     # Notify Frontend: "Tool done"
                     yield f"data: {json.dumps({'type': 'tool_end', 'result': result_str})}\n\n"
                     
                     messages.append({
                         "role": "tool",
                         "tool_call_id": call_id,
                         "content": str(result_str)
                     })
                     
                 except Exception as e:
                     err_msg = f"Tool Execution Failed: {str(e)}"
                     yield f"data: {json.dumps({'type': 'error', 'message': err_msg})}\n\n"
                     messages.append({"role": "tool", "tool_call_id": call_id, "content": err_msg})

             # --- Round 3: Final Answer based on Tool Results ---
             stream2 = client.chat.completions.create(
                model=request.model,
                messages=messages,
                stream=True
             )
             
             for chunk in stream2:
                 yield f"data: {json.dumps(chunk.model_dump())}\n\n"
                 
        yield "data: [DONE]\n\n"

    except Exception as e:
        print(f"Server Error: {e}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest, req: Request):
    return StreamingResponse(smart_stream_generator(request, req), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)
