
# ==========================================
# FILE: backend/server.py
# ==========================================

import os
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import json
import asyncio
from openai import OpenAI

# Import local MCP Manager
from mcp_manager import MCPManager

app = FastAPI(title="DellTech AI Workspace API", version="2.0.0")

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize MCP Manager
mcp_manager = MCPManager(config_path="mcp_config.json")

# Models
class Message(BaseModel):
    role: str
    content: str | None
    tool_call_id: str | None = None

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    stream: bool = False
    temperature: float = 0.7

class AddServerRequest(BaseModel):
    name: str
    command: str
    args: List[str] = []
    env: Dict[str, str] = {}

class ToggleServerRequest(BaseModel):
    enabled: bool

@app.on_event("startup")
async def startup_event():
    """Start MCP servers on backend startup with auto-discovery."""
    print("--- DellTech AI Backend Starting ---")
    
    # --- Auto-Configuration Logic ---
    config_path = "mcp_config.json"
    should_configure = False
    
    if not os.path.exists(config_path):
        should_configure = True
    else:
        try:
            with open(config_path, 'r') as f:
                data = json.load(f)
                if not data.get("servers"): should_configure = True
        except:
            should_configure = True

    if should_configure:
        print("[System] Configuring default MCP servers...")
        cwd = os.getcwd()
        # Look for the script in current or backend folder
        candidates = [
            os.path.join(cwd, "backend", "outlook_MCP_server.py"),
            os.path.join(cwd, "outlook_MCP_server.py")
        ]
        
        script_path = next((p for p in candidates if os.path.exists(p)), None)
        
        if script_path:
            # Normalize path for JSON
            script_path = script_path.replace("\\", "/")
            default_config = {
                "servers": {
                    "Outlook-Sync": {
                        "command": "python",
                        "args": [script_path],
                        "enabled": True,
                        "env": {}
                    }
                }
            }
            with open(config_path, "w") as f:
                json.dump(default_config, f, indent=2)
            print(f"[System] Auto-configured Outlook MCP at {script_path}")
        else:
            print("[System] Outlook script not found. Skipping auto-config.")
            
    await mcp_manager.start_all()

@app.on_event("shutdown")
async def shutdown_event():
    await mcp_manager.stop_all()

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.get("/mcp/servers")
async def list_servers():
    return mcp_manager.list_servers_status()

@app.post("/mcp/servers")
async def add_server(req: AddServerRequest):
    await mcp_manager.add_server(req.name, {"command": req.command, "args": req.args, "env": req.env, "enabled": True})
    return {"status": "added", "name": req.name}

@app.patch("/mcp/servers/{name}")
async def toggle_server(name: str, req: ToggleServerRequest):
    await mcp_manager.toggle_server(name, req.enabled)
    return {"status": "updated", "name": name, "enabled": req.enabled}

@app.delete("/mcp/servers/{name}")
async def remove_server(name: str):
    await mcp_manager.remove_server(name)
    return {"status": "removed", "name": name}

# --- Core Chat Logic ---

async def smart_stream_generator(request: ChatCompletionRequest, req: Request):
    """
    The Brain. Orchestrates the conversation between User, LLM, and MCP Tools.
    """
    api_key = req.headers.get("Authorization", "").replace("Bearer ", "")
    llm_base_url = req.headers.get("X-LLM-Base-URL", "https://api.openai.com/v1")
    
    if not api_key:
        yield f"data: {json.dumps({'error': 'API Key Missing'})}\n\n"
        return

    try:
        client = OpenAI(api_key=api_key, base_url=llm_base_url)
        
        # 1. Fetch available tools dynamically from MCP Manager
        tools = await mcp_manager.get_all_tools_definitions()
        
        messages = [{"role": m.role, "content": m.content, "tool_call_id": m.tool_call_id} for m in request.messages]
        
        # System Prompt Injection if not present
        if messages and messages[0]['role'] != 'system':
            messages.insert(0, {"role": "system", "content": "You are a helpful Dell Tech assistant. You have access to tools. Use them whenever data is needed (email, calendar)."})

        # --- Round 1: Initial Query ---
        stream = client.chat.completions.create(
            model=request.model,
            messages=messages,
            tools=tools if tools else None,
            stream=True,
            temperature=request.temperature
        )
        
        tool_calls = []
        
        for chunk in stream:
            delta = chunk.choices[0].delta
            
            # Accumulate Tool Calls
            if delta.tool_calls:
                for tc in delta.tool_calls:
                    if tc.index is not None:
                         while len(tool_calls) <= tc.index:
                             tool_calls.append({"id": "", "function": {"name": "", "arguments": ""}})
                         if tc.id: tool_calls[tc.index]["id"] = tc.id
                         if tc.function.name: tool_calls[tc.index]["function"]["name"] = tc.function.name
                         if tc.function.arguments: tool_calls[tc.index]["function"]["arguments"] += tc.function.arguments
            
            # Yield Content immediately to user
            if delta.content:
                yield f"data: {json.dumps(chunk.model_dump())}\n\n"

        # --- Round 2: Tool Execution (if needed) ---
        if tool_calls:
             # Append the Assistant's "Thought" (the tool call) to history
             messages.append({
                 "role": "assistant",
                 "content": None,
                 "tool_calls": tool_calls
             })
             
             for tc in tool_calls:
                 func_name = tc["function"]["name"]
                 call_id = tc["id"]
                 
                 # Notify Frontend: "I am starting a tool"
                 yield f"data: {json.dumps({'type': 'tool_start', 'tool': func_name})}\n\n"
                 
                 try:
                     args = json.loads(tc["function"]["arguments"])
                     print(f"[Orchestrator] Executing {func_name} with {args}")
                     
                     # CALL MCP
                     result_str = await mcp_manager.execute_tool(func_name, args)
                     
                     # Notify Frontend: "Tool done"
                     yield f"data: {json.dumps({'type': 'tool_end', 'result': result_str})}\n\n"
                     
                     messages.append({
                         "role": "tool",
                         "tool_call_id": call_id,
                         "content": str(result_str)
                     })
                     
                 except Exception as e:
                     err_msg = f"Tool Execution Failed: {str(e)}"
                     yield f"data: {json.dumps({'type': 'error', 'message': err_msg})}\n\n"
                     messages.append({"role": "tool", "tool_call_id": call_id, "content": err_msg})

             # --- Round 3: Final Answer based on Tool Results ---
             stream2 = client.chat.completions.create(
                model=request.model,
                messages=messages,
                stream=True
             )
             
             for chunk in stream2:
                 yield f"data: {json.dumps(chunk.model_dump())}\n\n"
                 
        yield "data: [DONE]\n\n"

    except Exception as e:
        print(f"Server Error: {e}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest, req: Request):
    return StreamingResponse(smart_stream_generator(request, req), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)


# ==========================================
# FILE: backend/mcp_manager.py
# ==========================================

import asyncio
import json
import os
import shutil
from typing import Dict, Any, List, Optional
import sys

class MCPManager:
    def __init__(self, config_path: str = "mcp_config.json"):
        self.config_path = config_path
        self.processes: Dict[str, asyncio.subprocess.Process] = {}
        self.config = self._load_config()
        self.tool_cache = {}

    def _load_config(self) -> Dict[str, Any]:
        if not os.path.exists(self.config_path):
            return {"servers": {}}
        try:
            with open(self.config_path, 'r') as f:
                return json.load(f)
        except:
            return {"servers": {}}

    def save_config(self):
        with open(self.config_path, 'w') as f:
            json.dump(self.config, f, indent=2)

    async def add_server(self, name: str, config: Dict):
        self.config["servers"][name] = config
        self.save_config()
        await self.start_server(name, config)

    async def toggle_server(self, name: str, enabled: bool):
        if name in self.config.get("servers", {}):
            self.config["servers"][name]["enabled"] = enabled
            self.save_config()
            
            if enabled:
                await self.start_server(name, self.config["servers"][name])
            else:
                await self.stop_server(name)

    async def remove_server(self, name: str):
        await self.stop_server(name)
        
        if "servers" in self.config and name in self.config["servers"]:
            del self.config["servers"][name]
            self.save_config()
            
        if name in self.tool_cache:
            del self.tool_cache[name]

    async def start_all(self):
        if "servers" not in self.config: self.config["servers"] = {}
        for name, cfg in self.config.get("servers", {}).items():
            if cfg.get("enabled", True):
                await self.start_server(name, cfg)

    async def start_server(self, name: str, config: Dict):
        if name in self.processes: return
        
        cmd = config["command"]
        args = config["args"]
        env = os.environ.copy()
        env.update(config.get("env", {}))
        
        print(f"[MCP] Launching {name}...")
        try:
            # Platform specific python command resolution
            if cmd == "python":
                if shutil.which("python3"): cmd = "python3"
                elif shutil.which("python"): cmd = "python"
                
            process = await asyncio.create_subprocess_exec(
                cmd, *args,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )
            self.processes[name] = process
            
            # Initial handshake/discovery
            await self._refresh_tools(name)
            
        except Exception as e:
            print(f"[MCP] Failed to start {name}: {e}")

    async def stop_server(self, name: str):
        if name in self.processes:
            try:
                self.processes[name].terminate()
                await self.processes[name].wait()
            except: pass
            del self.processes[name]
            # Clear cache when stopped so tools are not advertised
            if name in self.tool_cache:
                del self.tool_cache[name]

    async def stop_all(self):
        for name in list(self.processes.keys()):
            await self.stop_server(name)

    # --- Communication Layer ---

    async def _send_json_rpc(self, process, method: str, params: Any = None, id: int = 1):
        if not process.stdin: return None
        
        req = {"jsonrpc": "2.0", "method": method, "params": params or {}, "id": id}
        try:
            data = json.dumps(req).encode() + b"\n"
            process.stdin.write(data)
            await process.stdin.drain()
            
            # Read response (Assuming line-delimited JSON-RPC for this simple implementation)
            line = await process.stdout.readline()
            if not line: return None
            return json.loads(line.decode())
        except Exception as e:
            print(f"[MCP] RPC Error: {e}")
            return None

    async def _refresh_tools(self, server_name: str):
        proc = self.processes.get(server_name)
        if not proc: return
        
        resp = await self._send_json_rpc(proc, "tools/list")
        if resp and "result" in resp:
            tools = resp["result"].get("tools", [])
            self.tool_cache[server_name] = tools
            print(f"[MCP] {server_name} registered {len(tools)} tools.")

    async def get_all_tools_definitions(self) -> List[Dict]:
        """Convert MCP tool definitions to OpenAI format"""
        definitions = []
        for server, tools in self.tool_cache.items():
            for tool in tools:
                definitions.append({
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool.get("description", ""),
                        "parameters": tool.get("inputSchema", {})
                    }
                })
        return definitions

    async def execute_tool(self, tool_name: str, args: Dict) -> str:
        # Find provider
        server_name = next((s for s, tools in self.tool_cache.items() if any(t['name'] == tool_name for t in tools)), None)
        if not server_name: return "Tool not found."
            
        proc = self.processes.get(server_name)
        if not proc: return "Server not active."
        
        resp = await self._send_json_rpc(proc, "tools/call", {"name": tool_name, "arguments": args})
        
        if resp and "result" in resp:
            # Handle MCP content list (text/image) -> Flatten to text for LLM
            content = resp["result"].get("content", [])
            text_content = [c["text"] for c in content if c.get("type") == "text"]
            return "\n".join(text_content)
        elif resp and "error" in resp:
             return f"Error: {resp['error'].get('message')}"
             
        return "Empty response."

    def list_servers_status(self):
        return [{
            "id": k, 
            "name": k, 
            "status": "running" if k in self.processes else "stopped",
            "tools": self.tool_cache.get(k, [])
        } for k in self.config.get("servers", {})]


# ==========================================
# FILE: backend/outlook_MCP_server.py
# ==========================================

import sys
import json
import logging
import os

# Ensure win32com is installed: pip install pywin32
try:
    import pythoncom
    import win32com.client
except ImportError:
    # Graceful fallback for non-Windows dev environments
    pythoncom = None
    win32com = None

logging.basicConfig(level=logging.ERROR)

def get_outlook_messages(count=5):
    if not pythoncom:
        return [{"error": "win32com library not found (Windows only)."}]
        
    try:
        # Crucial for running in a secondary process/thread
        pythoncom.CoInitialize()
        
        outlook = win32com.client.Dispatch("Outlook.Application").GetNamespace("MAPI")
        inbox = outlook.GetDefaultFolder(6) # 6 = Inbox
        items = inbox.Items
        items.Sort("[ReceivedTime]", True)
        
        results = []
        # Loop with safety limit
        for i in range(min(count, 20)):
            try:
                msg = items[i]
                results.append({
                    "subject": msg.Subject,
                    "sender": msg.SenderName,
                    "preview": msg.Body[:150].replace("\r\n", " ") + "..."
                })
            except Exception:
                continue
        return results
    except Exception as e:
        return [{"error": f"Outlook Access Failed: {str(e)}"}]

def handle_request(line):
    try:
        req = json.loads(line)
        method = req.get("method")
        msg_id = req.get("id")
        
        response = {"jsonrpc": "2.0", "id": msg_id}

        if method == "tools/list":
            response["result"] = {
                "tools": [{
                    "name": "read_outlook_emails",
                    "description": "Fetch recent emails from the user's local Outlook Inbox.",
                    "inputSchema": {
                        "type": "object",
                        "properties": {
                            "count": {"type": "integer", "description": "Number of emails to fetch (default 5)"}
                        }
                    }
                }]
            }
            
        elif method == "tools/call":
            params = req.get("params", {})
            name = params.get("name")
            args = params.get("arguments", {})
            
            if name == "read_outlook_emails":
                data = get_outlook_messages(args.get("count", 5))
                response["result"] = {
                    "content": [{"type": "text", "text": json.dumps(data, indent=2)}]
                }
            else:
                response["error"] = {"code": -32601, "message": "Method not found"}
        
        else:
            return # Ignore irrelevant messages

        sys.stdout.write(json.dumps(response) + "\n")
        sys.stdout.flush()
        
    except Exception as e:
        logging.error(f"Handler Error: {e}")

if __name__ == "__main__":
    # Unbuffered IO for instant MCP communication
    if sys.platform == 'win32':
        import msvcrt
        msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)
        msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)

    for line in sys.stdin:
        if line.strip():
            handle_request(line)
